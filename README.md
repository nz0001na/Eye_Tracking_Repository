# Eye_Tracking_Repository
This repository collects methods and databases in the field of eye tracking.

Keep updating!!!

[[report]](https://github.com/nz0001na/Eye_Tracking_Repository/blob/main/literature_review.pdf)

# Survey
* 2018	Eye Tracking Measures in Aviation A Selective Literature Review [[paper]](https://www.tandfonline.com/doi/full/10.1080/24721840.2018.1514978)
* 2021	Application of Eye Tracking Technology in Aviation, Maritime, and Construction Industries_A Systematic Review [[paper]](https://www.mdpi.com/1424-8220/21/13/4289)
* 2020	A Review of Various State of Art Eye Gaze Estimation Techniques [[paper]](https://link.springer.com/chapter/10.1007/978-981-15-1275-9_41?awc=26429_1695841396_6341dd51c3ff775c846f499409bd817d&utm_medium=affiliate&utm_source=awin&utm_campaign=CONR_BOOKS_ECOM_DE_PHSS_ALWYS_DEEPLINK&utm_content=textlink&utm_term=922583)
* 2021	A survey on Deep Learning Based Eye Gaze Estimation Methods [[paper]](https://www.researchgate.net/profile/Sangeetha_Skb/publication/354452039_A_survey_on_Deep_Learning_Based_Eye_Gaze_Estimation_Methods/links/62387239d1e27a083bc21993/A-survey-on-Deep-Learning-Based-Eye-Gaze-Estimation-Methods.pdf?_sg%5B0%5D=started_experiment_milestone&origin=journalDetail)
* 2022	Eye gaze estimation: A survey on deep learning-based approaches	 [[paper]](https://www.sciencedirect.com/science/article/abs/pii/S0957417422003347)
* 2021	Appearance-based Gaze Estimation With Deep Learning: A Review and Benchmark	[[website]](https://arxiv.org/pdf/2104.12668),	[[paper]](https://arxiv.org/pdf/2104.12668)
* 2021	A survey on Deep Learning Based Eye Gaze Estimation Methods	[[paper]](https://www.researchgate.net/profile/Sangeetha_Skb/publication/354452039_A_survey_on_Deep_Learning_Based_Eye_Gaze_Estimation_Methods/links/62387239d1e27a083bc21993/A-survey-on-Deep-Learning-Based-Eye-Gaze-Estimation-Methods.pdf?_sg%5B0%5D=started_experiment_milestone&origin=journalDetail)
* 2022	Eye-Tracking Technologies in Mobile Devices Using Edge Computing: A Systematic Review		[[paper]](https://www.researchgate.net/profile/Sangeetha_Skb/publication/354452039_A_survey_on_Deep_Learning_Based_Eye_Gaze_Estimation_Methods/links/62387239d1e27a083bc21993/A-survey-on-Deep-Learning-Based-Eye-Gaze-Estimation-Methods.pdf?_sg%5B0%5D=started_experiment_milestone&origin=journalDetail)
* 2024	Appearance-based Gaze Estimation With Deep Learning: A Review and Benchmark		[[paper]](https://arxiv.org/pdf/2104.12668)		
* 2023	An End-to-End Review of Gaze Estimation and its Interactive Applications on Handheld Mobile Devices		[[paper]](https://arxiv.org/pdf/2104.12668)		
* 2023	Automatic Gaze Analysis: A Survey of Deep Learning based Approaches	[[project]](https://arxiv.org/pdf/2104.12668),	[[paper]](https://arxiv.org/pdf/2104.12668)		
* 2023	Eye-tracking paradigms for the assessment of mild cognitive impairment: a systematic review		[[paper]](https://arxiv.org/pdf/2104.12668)		
* 2023	Application of eye-tracking systems integrated into immersive virtual reality and possible transfer to the sports sector - A systematic review		[[paper]](https://arxiv.org/pdf/2104.12668)
* 2023	Webcam Eye Tracking for Desktop and Mobile Devices: A Systematic Review		[[paper]](https://arxiv.org/pdf/2104.12668),	[[paper]](https://arxiv.org/pdf/2104.12668)




# Databases
* 2013	CAVE  [[paper]](https://dl.acm.org/doi/10.1145/2501988.2501994),	[[project]](https://www.cs.columbia.edu/CAVE/projects/gaze_locking/index.php),	[[Data]](https://www.cs.columbia.edu/CAVE/databases/columbia_gaze/)
* 2014	EYEDIAP  [[paper]](https://dl.acm.org/doi/10.1145/2578153.2578190),	[[project]](https://paperswithcode.com/dataset/eyediap),	[[Database]](https://www.idiap.ch/en/scientific-research/data/eyediap)
* 2014	UT MV  [[Database]](https://www.ut-vision.org/datasets/)
* 2015	OMEG  [[paper]](https://tiedejatutkimus.fi/fi/results/publication/0006190515)
* 2015	MPIIGaze		[[paper]](https://paperswithcode.com/paper/appearance-based-gaze-estimation-in-the-wild),	[[project]](https://paperswithcode.com/dataset/mpiigaze)
* 2015	GazeFollow  [[paper]](https://proceedings.neurips.cc/paper_files/paper/2015/file/ec8956637a99787bd197eacd77acce5e-Paper.pdf), [[project]](http://gazefollow.csail.mit.edu/)
* 2015	SynthesEye  [[paper]](https://openaccess.thecvf.com/content_iccv_2015/papers/Wood_Rendering_of_Eyes_ICCV_2015_paper.pdf), [[project]](https://www.cl.cam.ac.uk/research/rainbow/projects/syntheseyes/)
* 2016	GazeCapture		[[paper]](https://paperswithcode.com/paper/eye-tracking-for-everyone),	[[project]](https://paperswithcode.com/dataset/gazecapture)
* 2016	UnityEyes  [[paper]](https://dl.acm.org/doi/10.1145/2857491.2857492), [[project]](https://www.cl.cam.ac.uk/research/rainbow/projects/unityeyes/)
* 2017	TabletGaze  [[paper]](https://arxiv.org/abs/1508.01244), [[project]](https://sh.rice.edu/cognitive-engagement/tabletgaze/)
* 2017	MPIIFaceGaze	[[project]](https://www.perceptualui.org/research/datasets/MPIIFaceGaze/)	
* 2017	InvisibleEye  [[paper]](https://dl.acm.org/doi/10.1145/3130971)		
* 2018	RT-GENE		[[paper]](https://paperswithcode.com/paper/rt-gene-real-time-eye-gaze-estimation-in),	[[project]](https://paperswithcode.com/dataset/rt-gene)
* 2019	Gaze360		[[paper]](https://paperswithcode.com/paper/gaze360-physically-unconstrained-gaze),	[[project]](https://paperswithcode.com/dataset/gaze360)
* 2019	RT-BENE  [[paper]](https://paperswithcode.com/paper/rt-gene-real-time-eye-gaze-estimation-in), [[project]](https://paperswithcode.com/dataset/rt-gene), [[database]](https://github.com/Tobias-Fischer/rt_gene)
* 2019	 NV Gaze  [[paper]](https://dl.acm.org/doi/10.1145/3290605.3300780), [[project]](https://sites.google.com/nvidia.com/nvgaze), [[database]](https://research.nvidia.com/publication/2019-05_nvgaze-anatomically-informed-dataset-low-latency-near-eye-gaze-estimation)
* 2019	HUST-LEBW  [[paper]](https://arxiv.org/abs/1902.07891),  [[project]](https://github.com/thorhu/Eyeblink-in-the-wild) 
* 2019	VACATION  [[paper]](https://openaccess.thecvf.com/content_ICCV_2019/papers/Fan_Understanding_Human_Gaze_Communication_by_Spatio-Temporal_Graph_Reasoning_ICCV_2019_paper.pdf), [[project]](https://github.com/LifengFan/Human-Gaze-Communication)
* 2020	mEBAL  [[paper]](https://arxiv.org/abs/2006.05327), [[project]](https://github.com/BiDAlab/mEBAL)
* 2020	ETH-XGaze		[[paper]](https://paperswithcode.com/paper/eth-xgaze-a-large-scale-dataset-for-gaze),	[[project]](https://paperswithcode.com/dataset/eth-xgaze), [[data]](https://ait.ethz.ch/xgaze)
* 2020	EVE		[[paper]](https://paperswithcode.com/paper/towards-end-to-end-video-based-eye-tracking),	[[project]](https://paperswithcode.com/dataset/eve), [[code]](https://github.com/swook/EVE)
* 2020	Gaze-in-the-Wild  [[paper]](https://arxiv.org/abs/1905.13146), [[project]](https://www.cis.rit.edu/~rsk3900/gaze-in-wild/)
* 2021	LAEO  [[paper]](https://arxiv.org/abs/2105.09803), [[paper]](https://openaccess.thecvf.com/content/CVPR2021/papers/Kothari_Weakly-Supervised_Physically_Unconstrained_Gaze_Estimation_CVPR_2021_paper.pdf), [[project]](https://github.com/AVAuco/ucolaeodb)
* 2021	GOO [[paper]](https://openaccess.thecvf.com/content/CVPR2021W/GAZE/papers/Tomas_GOO_A_Dataset_for_Gaze_Object_Prediction_in_Retail_Environments_CVPRW_2021_paper.pdf), [[project]](https://github.com/upeee/GOO-GAZE2021), [[database]]() 
* 2021	OpenNEEDS [[paper]](https://www.researchgate.net/publication/351861290_OpenNEEDS_A_Dataset_of_Gaze_Head_Hand_and_Scene_Signals_During_Exploration_in_Open-Ended_VR_Environments), [[project]](https://research.facebook.com/publications/openneeds-a-dataset-of-gaze-head-hand-and-scene-signals-during-exploration-in-open-ended-vr-environments/)
* 2023	GFIE  [[paper]](https://openaccess.thecvf.com/content/CVPR2023/papers/Hu_GFIE_A_Dataset_and_Baseline_for_Gaze-Following_From_2D_to_CVPR_2023_paper.pdf), [[project]](https://sites.google.com/view/gfie)
* 2020	VideoAttentionTarget  [[paper]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Chong_Detecting_Attended_Visual_Targets_in_Video_CVPR_2020_paper.pdf), [[project]](https://github.com/ejcgt/attention-target-detection), [[database]](https://www.dropbox.com/s/8ep3y1hd74wdjy5/videoattentiontarget.zip?e=1&dl=0) 







# Algorithms
## Image-based methods
* 2015  Appearance-Based Gaze Estimation in the Wild, CVPR 2015 [[paper]](https://openaccess.thecvf.com/content_cvpr_2015/html/Zhang_Appearance-Based_Gaze_Estimation_2015_CVPR_paper.html), [[code]](https://github.com/yihuacheng/Mnist)
* 2016 Eye Tracking for Everyone [[paper]](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Krafka_Eye_Tracking_for_CVPR_2016_paper.html), [[code]](https://github.com/yihuacheng/Itracker), [[project]](https://gazecapture.csail.mit.edu/)
* 2017  Itâ€™s written all over your face: Full-face appearance-based gaze estimation, CVPRW 2017  [[paper]](https://openaccess.thecvf.com/content_cvpr_2017_workshops/w41/html/Bulling_Its_Written_All_CVPR_2017_paper.html?ref=https://githubhelp.com), [[code]](https://github.com/yihuacheng/Full-face)
* 2017  MPIIGaze: Real-World Dataset and Deep Appearance-Based Gaze Estimation, TPAMI 2017 [[paper]](https://ieeexplore.ieee.org/abstract/document/8122058), [[code]](https://github.com/yihuacheng/Gaze-Net)
* 2018  Appearance-Based Gaze Estimation via Evaluation-Guided Asymmetric Regression [[paper]](https://openaccess.thecvf.com/content_ECCV_2018/html/Yihua_Cheng_Appearance-Based_Gaze_Estimation_ECCV_2018_paper.html), [[code]](https://github.com/yihuacheng/ARE-GazeEstimation)
* 2018	[ELG]	Learning to Find Eye Region Landmarks for Remote Gaze Estimation in Unconstrained Settings	[[paper]](https://arxiv.org/abs/1805.04771), 	[[code]](https://github.com/swook/GazeML),	[[project]](https://ait.ethz.ch/projects/2018/landmarks-gaze/)
* 2018	[DPG]	Deep Pictorial Gaze Estimation	[[paper]](https://openaccess.thecvf.com/content_ECCV_2018/papers/Seonwook_Park_Deep_Pictorial_Gaze_ECCV_2018_paper.pdf),	[[code]](https://github.com/swook/GazeML),	[[project]](https://ait.ethz.ch/projects/2018/pictorial-gaze/)
* 2018  RT-GENE: Real-Time Eye Gaze Estimation in Natural Environments [[paper]](https://openaccess.thecvf.com/content_ECCV_2018/html/Tobias_Fischer_RT-GENE_Real-Time_Eye_ECCV_2018_paper.html), [[code]](https://github.com/yihuacheng/RT-Gene)
* 2019  Appearance-Based Gaze Estimation Using Dilated-Convolutions [[paper]](https://link.springer.com/chapter/10.1007/978-3-030-20876-9_20), [[code]](https://github.com/yihuacheng/Dilated-Net)
* 2020		Unsupervised Representation Learning for Gaze Estimation	[[paper]](https://openaccess.thecvf.com/content_CVPR_2020/html/Yu_Unsupervised_Representation_Learning_for_Gaze_Estimation_CVPR_2020_paper.html)
* 2021		Generalizing Gaze Estimation with Outlier-guided Collaborative Adaptation	[[paper]](https://openaccess.thecvf.com/content/ICCV2021/html/Liu_Generalizing_Gaze_Estimation_With_Outlier-Guided_Collaborative_Adaptation_ICCV_2021_paper.html),	[[code]](https://github.com/DreamtaleCore/PnP-GA),	[[project]](https://liuyunfei.net/publication/iccv2021_pnp-ga/)
* 2021		Gaze Estimation using Transformer	[[paper]](https://arxiv.org/abs/2105.14424),	[[code]](https://github.com/yihuacheng/GazeTR)	
* 2021		FLAME: Facial Landmark Heatmap Activated Multimodal Gaze Estimation	[[paper]](https://arxiv.org/abs/2110.04828),	[[code]](https://github.com/neelabhsinha/flame)	
* 2021		High-Accuracy Gaze Estimation for Interpolation-Based Eye-Tracking Methods	[[paper]](https://www.mdpi.com/2411-5150/5/3/41)	
* 2021		Real-Time Precise Human-Computer Interaction System Based on Gaze Estimation and Tracking	[[paper]](https://www.hindawi.com/journals/wcmc/2021/8213946/)
* 2021		Effect Of Personalized Calibration On Gaze Estimation Using Deep-Learning	 [[paper]](https://arxiv.org/abs/2109.12801)
* 2021		Weakly-Supervised Physically Unconstrained Gaze Estimation	[[paper]](https://arxiv.org/pdf/2105.09803.pdf),	[[code]](https://github.com/NVlabs/weakly-supervised-gaze)
* 2021		Gaze Estimation with an Ensemble of Four Architectures	[[paper]](https://arxiv.org/abs/2107.01980)
* 2021	The Story in Your Eyes: An Individual-difference-aware Model for Cross-person Gaze Estimation	[[paper]](https://arxiv.org/abs/2106.14183),	[[code]](https://github.com/bjj9/EVE_SCPT)	
* 2021	[Puregaze]	Puregaze: Purifying gaze feature for generalizable gaze estimation	[[paper]](https://arxiv.org/abs/2103.13173),	[[code]](https://github.com/yihuacheng/puregaze)	
* 2021	HybridGazeNet	HybridGazeNet: Geometric model guided Convolutional Neural Networks for gaze estimation	[[paper]](https://arxiv.org/abs/2111.11691)		
* 2022		Learning-by-Novel-View-Synthesis for Full-Face Appearance-based 3D Gaze Estimation	[[paper]](https://arxiv.org/abs/2201.07927)		
* 2022	[MTGLS]	MTGLS: Multi-Task Gaze Estimation with Limited Supervision	[[paper]](https://openaccess.thecvf.com/content/WACV2022/html/Ghosh_MTGLS_Multi-Task_Gaze_Estimation_With_Limited_Supervision_WACV_2022_paper.html)		


## Video-based methods

* 2018		Recurrent cnn for 3d gaze estimation using appearance and shape cues  [[paper]](Recurrent cnn for 3d gaze estimation using appearance and shape cues)		
* 2019		Neuro-inspired eye tracking with eye movement dynamics	[[paper]](https://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Neuro-Inspired_Eye_Tracking_With_Eye_Movement_Dynamics_CVPR_2019_paper.html)
* 2019		Realtime and accurate 3d eye gaze capture with dcnn-based iris and pupil segmentation	[[paper]](https://ieeexplore.ieee.org/abstract/document/8818661)
* 2019		Learning a 3d gaze estimator with improved itracker combined with bidirectional lstm  [[paper]](https://ieeexplore.ieee.org/abstract/document/8784770)				
* 2019	[Gaze360]	Gaze360: Physically unconstrained gaze estimation in the wild	[[paper]](http://gaze360.csail.mit.edu/iccv2019_gaze360.pdf),	[[code]](https://github.com/Erkil1452/gaze360),	[[code2]](https://github.com/erkil1452/gaze360/tree/master/code),	[[project]](http://gaze360.csail.mit.edu/)
* 2020  A Coarse-to-Fine Adaptive Network for Appearance-Based Gaze Estimation [[paper]](https://ojs.aaai.org/index.php/AAAI/article/view/6636),
* 2020	Towards end-to-end video-based eye-tracking	[[paper]](https://arxiv.org/abs/2007.13120),	[[code]](https://github.com/swook/EVE)		
* 2022	[Puregaze]	Puregaze: Purifying gaze feature for generalizable gaze estimation	[[paper]](https://ojs.aaai.org/index.php/AAAI/article/view/19921),	[[code]](https://github.com/yihuacheng/PureGaze)	
* 2022	[HAZE-Net]	HAZE-Net: High-Frequency Attentive Super-Resolved Gaze Estimation in Low-Resolution Face Images	[[paper]](https://arxiv.org/abs/2209.10167),	[[code]](https://github.com/dbseorms16/haze_net)		
* 2022	[LatentGaze]	LatentGaze: Cross-Domain Gaze Estimation through Gaze-Aware Analytic Latent Code Manipulation	[[paper]](https://arxiv.org/abs/2209.10171),	[[code]](https://github.com/leeisack/latentgaze)		

# Other codes
* https://github.com/antoinelame/GazeTracking
* https://github.com/iitmcvg/eye-gaze

