# Eye_Tracking_Repository
This repository collects methods and databases in the field of eye tracking.

Keep updating!!!

[[report]](https://github.com/nz0001na/Eye_Tracking_Repository/blob/main/literature_review.pdf)

# Survey
* 2018	Eye Tracking Measures in Aviation A Selective Literature Review [[paper]](https://www.tandfonline.com/doi/full/10.1080/24721840.2018.1514978)
* 2021	Application of Eye Tracking Technology in Aviation, Maritime, and Construction Industries_A Systematic Review [[paper]](https://www.mdpi.com/1424-8220/21/13/4289)
* 2020	A Review of Various State of Art Eye Gaze Estimation Techniques [[paper]](https://link.springer.com/chapter/10.1007/978-981-15-1275-9_41?awc=26429_1695841396_6341dd51c3ff775c846f499409bd817d&utm_medium=affiliate&utm_source=awin&utm_campaign=CONR_BOOKS_ECOM_DE_PHSS_ALWYS_DEEPLINK&utm_content=textlink&utm_term=922583)
* 2021	A survey on Deep Learning Based Eye Gaze Estimation Methods [[paper]](https://www.researchgate.net/profile/Sangeetha_Skb/publication/354452039_A_survey_on_Deep_Learning_Based_Eye_Gaze_Estimation_Methods/links/62387239d1e27a083bc21993/A-survey-on-Deep-Learning-Based-Eye-Gaze-Estimation-Methods.pdf?_sg%5B0%5D=started_experiment_milestone&origin=journalDetail)
* 2022	Eye gaze estimation: A survey on deep learning-based approaches	 [[paper]](https://www.sciencedirect.com/science/article/abs/pii/S0957417422003347)
* 2021	Appearance-based Gaze Estimation With Deep Learning: A Review and Benchmark	[[website]](https://arxiv.org/pdf/2104.12668),	[[paper]](https://arxiv.org/pdf/2104.12668)
* 2021	A survey on Deep Learning Based Eye Gaze Estimation Methods	[[paper]](https://www.researchgate.net/profile/Sangeetha_Skb/publication/354452039_A_survey_on_Deep_Learning_Based_Eye_Gaze_Estimation_Methods/links/62387239d1e27a083bc21993/A-survey-on-Deep-Learning-Based-Eye-Gaze-Estimation-Methods.pdf?_sg%5B0%5D=started_experiment_milestone&origin=journalDetail)
* 2022	Eye-Tracking Technologies in Mobile Devices Using Edge Computing: A Systematic Review		[[paper]](https://www.researchgate.net/profile/Sangeetha_Skb/publication/354452039_A_survey_on_Deep_Learning_Based_Eye_Gaze_Estimation_Methods/links/62387239d1e27a083bc21993/A-survey-on-Deep-Learning-Based-Eye-Gaze-Estimation-Methods.pdf?_sg%5B0%5D=started_experiment_milestone&origin=journalDetail)
* 2024	Appearance-based Gaze Estimation With Deep Learning: A Review and Benchmark		[[paper]](https://arxiv.org/pdf/2104.12668)		
* 2023	An End-to-End Review of Gaze Estimation and its Interactive Applications on Handheld Mobile Devices		[[paper]](https://arxiv.org/pdf/2104.12668)		
* 2023	Automatic Gaze Analysis: A Survey of Deep Learning based Approaches	[[project]](https://arxiv.org/pdf/2104.12668),	[[paper]](https://arxiv.org/pdf/2104.12668)		
* 2023	Eye-tracking paradigms for the assessment of mild cognitive impairment: a systematic review		[[paper]](https://arxiv.org/pdf/2104.12668)		
* 2023	Application of eye-tracking systems integrated into immersive virtual reality and possible transfer to the sports sector - A systematic review		[[paper]](https://arxiv.org/pdf/2104.12668)
* 2023	Webcam Eye Tracking for Desktop and Mobile Devices: A Systematic Review		[[paper]](https://arxiv.org/pdf/2104.12668),	[[paper]](https://arxiv.org/pdf/2104.12668)




# Databases
* 2013	CAVE  [[paper]](https://dl.acm.org/doi/10.1145/2501988.2501994),	[[project]](https://www.cs.columbia.edu/CAVE/projects/gaze_locking/index.php),	[[Data]](https://www.cs.columbia.edu/CAVE/databases/columbia_gaze/)
* 2014	EYEDIAP  [[paper]](https://dl.acm.org/doi/10.1145/2578153.2578190),	[[project]](https://paperswithcode.com/dataset/eyediap),	[[Database]](https://www.idiap.ch/en/scientific-research/data/eyediap)
* 2014	UT MV  [[Database]](https://www.ut-vision.org/datasets/)
* 2015	OMEG  [[paper]](https://tiedejatutkimus.fi/fi/results/publication/0006190515)
* 2015	MPIIGaze		[[paper]](https://paperswithcode.com/paper/appearance-based-gaze-estimation-in-the-wild),	[[project]](https://paperswithcode.com/dataset/mpiigaze)
* 2015	GazeFollow  [[paper]](https://proceedings.neurips.cc/paper_files/paper/2015/file/ec8956637a99787bd197eacd77acce5e-Paper.pdf), [[project]](http://gazefollow.csail.mit.edu/)
* 2015	SynthesEye  [[paper]](https://openaccess.thecvf.com/content_iccv_2015/papers/Wood_Rendering_of_Eyes_ICCV_2015_paper.pdf), [[project]](https://www.cl.cam.ac.uk/research/rainbow/projects/syntheseyes/)
* 2016	GazeCapture		[[paper]](https://paperswithcode.com/paper/eye-tracking-for-everyone),	[[project]](https://paperswithcode.com/dataset/gazecapture)
* 2016	UnityEyes  [[paper]](https://dl.acm.org/doi/10.1145/2857491.2857492), [[project]](https://www.cl.cam.ac.uk/research/rainbow/projects/unityeyes/)
* 2017	TabletGaze  [[paper]](https://arxiv.org/abs/1508.01244), [[project]](https://sh.rice.edu/cognitive-engagement/tabletgaze/)
* 2017	MPIIFaceGaze	[[project]](https://www.perceptualui.org/research/datasets/MPIIFaceGaze/)	
* 2017	InvisibleEye  [[paper]](https://dl.acm.org/doi/10.1145/3130971)		
* 2018	RT-GENE		[[paper]](https://paperswithcode.com/paper/rt-gene-real-time-eye-gaze-estimation-in),	[[project]](https://paperswithcode.com/dataset/rt-gene)
* 2019	Gaze360		[[paper]](https://paperswithcode.com/paper/gaze360-physically-unconstrained-gaze),	[[project]](https://paperswithcode.com/dataset/gaze360)
* 2019	RT-BENE  [[paper]](https://paperswithcode.com/paper/rt-gene-real-time-eye-gaze-estimation-in), [[project]](https://paperswithcode.com/dataset/rt-gene), [[database]](https://github.com/Tobias-Fischer/rt_gene)
* 2019	 NV Gaze  [[paper]](https://dl.acm.org/doi/10.1145/3290605.3300780), [[project]](https://sites.google.com/nvidia.com/nvgaze), [[database]](https://research.nvidia.com/publication/2019-05_nvgaze-anatomically-informed-dataset-low-latency-near-eye-gaze-estimation)
* 2019	HUST-LEBW  [[paper]](https://arxiv.org/abs/1902.07891),  [[project]](https://github.com/thorhu/Eyeblink-in-the-wild) 
* 2019	VACATION  [[paper]](https://openaccess.thecvf.com/content_ICCV_2019/papers/Fan_Understanding_Human_Gaze_Communication_by_Spatio-Temporal_Graph_Reasoning_ICCV_2019_paper.pdf), [[project]](https://github.com/LifengFan/Human-Gaze-Communication)
* 2020	mEBAL  [[paper]](https://arxiv.org/abs/2006.05327), [[project]](https://github.com/BiDAlab/mEBAL)
* 2020	ETH-XGaze		[[paper]](https://paperswithcode.com/paper/eth-xgaze-a-large-scale-dataset-for-gaze),	[[project]](https://paperswithcode.com/dataset/eth-xgaze), [[data]](https://ait.ethz.ch/xgaze)
* 2020	EVE		[[paper]](https://paperswithcode.com/paper/towards-end-to-end-video-based-eye-tracking),	[[project]](https://paperswithcode.com/dataset/eve), [[code]](https://github.com/swook/EVE)
* 2020	Gaze-in-the-Wild  [[paper]](https://arxiv.org/abs/1905.13146), [[project]](https://www.cis.rit.edu/~rsk3900/gaze-in-wild/)
* 2021	LAEO  [[paper]](https://arxiv.org/abs/2105.09803), [[paper]](https://openaccess.thecvf.com/content/CVPR2021/papers/Kothari_Weakly-Supervised_Physically_Unconstrained_Gaze_Estimation_CVPR_2021_paper.pdf), [[project]](https://github.com/AVAuco/ucolaeodb)
* 2021	GOO [[paper]](https://openaccess.thecvf.com/content/CVPR2021W/GAZE/papers/Tomas_GOO_A_Dataset_for_Gaze_Object_Prediction_in_Retail_Environments_CVPRW_2021_paper.pdf), [[project]](https://github.com/upeee/GOO-GAZE2021), [[database]]() 
* 2021	OpenNEEDS [[paper]](https://www.researchgate.net/publication/351861290_OpenNEEDS_A_Dataset_of_Gaze_Head_Hand_and_Scene_Signals_During_Exploration_in_Open-Ended_VR_Environments), [[project]](https://research.facebook.com/publications/openneeds-a-dataset-of-gaze-head-hand-and-scene-signals-during-exploration-in-open-ended-vr-environments/)
* 2023	GFIE  [[paper]](https://openaccess.thecvf.com/content/CVPR2023/papers/Hu_GFIE_A_Dataset_and_Baseline_for_Gaze-Following_From_2D_to_CVPR_2023_paper.pdf), [[project]](https://sites.google.com/view/gfie)
* 2020	VideoAttentionTarget  [[paper]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Chong_Detecting_Attended_Visual_Targets_in_Video_CVPR_2020_paper.pdf), [[project]](https://github.com/ejcgt/attention-target-detection), [[database]](https://www.dropbox.com/s/8ep3y1hd74wdjy5/videoattentiontarget.zip?e=1&dl=0) 






# Algorithms
* 2015  Appearance-Based Gaze Estimation in the Wild, CVPR 2015 [[paper]](https://openaccess.thecvf.com/content_cvpr_2015/html/Zhang_Appearance-Based_Gaze_Estimation_2015_CVPR_paper.html), [[code]](https://github.com/yihuacheng/Mnist)
* 2016 Eye Tracking for Everyone [[paper]](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Krafka_Eye_Tracking_for_CVPR_2016_paper.html), [[code]](https://github.com/yihuacheng/Itracker), [[project]](https://gazecapture.csail.mit.edu/)
* 2017  It’s written all over your face: Full-face appearance-based gaze estimation, CVPRW 2017  [[paper]](https://openaccess.thecvf.com/content_cvpr_2017_workshops/w41/html/Bulling_Its_Written_All_CVPR_2017_paper.html?ref=https://githubhelp.com), [[code]](https://github.com/yihuacheng/Full-face)
* 2017  MPIIGaze: Real-World Dataset and Deep Appearance-Based Gaze Estimation, TPAMI 2017 [[paper]](https://ieeexplore.ieee.org/abstract/document/8122058), [[code]](https://github.com/yihuacheng/Gaze-Net)
* 2018  Appearance-Based Gaze Estimation via Evaluation-Guided Asymmetric Regression [[paper]](https://openaccess.thecvf.com/content_ECCV_2018/html/Yihua_Cheng_Appearance-Based_Gaze_Estimation_ECCV_2018_paper.html), [[code]](https://github.com/yihuacheng/ARE-GazeEstimation)
* 2018	[ELG]	Learning to Find Eye Region Landmarks for Remote Gaze Estimation in Unconstrained Settings	[[paper]](https://arxiv.org/abs/1805.04771), 	[[code]](https://github.com/swook/GazeML),	[[project]](https://ait.ethz.ch/projects/2018/landmarks-gaze/)
* 2018	[DPG]	Deep Pictorial Gaze Estimation	[[paper]](https://openaccess.thecvf.com/content_ECCV_2018/papers/Seonwook_Park_Deep_Pictorial_Gaze_ECCV_2018_paper.pdf),	[[code]](https://github.com/swook/GazeML),	[[project]](https://ait.ethz.ch/projects/2018/pictorial-gaze/)
* 2018  RT-GENE: Real-Time Eye Gaze Estimation in Natural Environments [[paper]](https://openaccess.thecvf.com/content_ECCV_2018/html/Tobias_Fischer_RT-GENE_Real-Time_Eye_ECCV_2018_paper.html), [[code]](https://github.com/yihuacheng/RT-Gene)
* 2019  Appearance-Based Gaze Estimation Using Dilated-Convolutions [[paper]](https://link.springer.com/chapter/10.1007/978-3-030-20876-9_20), [[code]](https://github.com/yihuacheng/Dilated-Net)
* 2020		Unsupervised Representation Learning for Gaze Estimation	[[paper]](https://openaccess.thecvf.com/content_CVPR_2020/html/Yu_Unsupervised_Representation_Learning_for_Gaze_Estimation_CVPR_2020_paper.html)
* 2021		Generalizing Gaze Estimation with Outlier-guided Collaborative Adaptation	[[paper]](https://openaccess.thecvf.com/content/ICCV2021/html/Liu_Generalizing_Gaze_Estimation_With_Outlier-Guided_Collaborative_Adaptation_ICCV_2021_paper.html),	[[code]](https://github.com/DreamtaleCore/PnP-GA),	[[project]](https://liuyunfei.net/publication/iccv2021_pnp-ga/)
* 2021		Gaze Estimation using Transformer	[[paper]](https://arxiv.org/abs/2105.14424),	[[code]](https://github.com/yihuacheng/GazeTR)	
* 2021		FLAME: Facial Landmark Heatmap Activated Multimodal Gaze Estimation	[[paper]](https://arxiv.org/abs/2110.04828),	[[code]](https://github.com/neelabhsinha/flame)	
* 2021		High-Accuracy Gaze Estimation for Interpolation-Based Eye-Tracking Methods	[[paper]](https://www.mdpi.com/2411-5150/5/3/41)	
* 2021		Real-Time Precise Human-Computer Interaction System Based on Gaze Estimation and Tracking	[[paper]](https://www.hindawi.com/journals/wcmc/2021/8213946/)
* 2021		Effect Of Personalized Calibration On Gaze Estimation Using Deep-Learning	 [[paper]](https://arxiv.org/abs/2109.12801)
* 2021		Weakly-Supervised Physically Unconstrained Gaze Estimation	[[paper]](https://arxiv.org/pdf/2105.09803.pdf),	[[code]](https://github.com/NVlabs/weakly-supervised-gaze)
* 2021		Gaze Estimation with an Ensemble of Four Architectures	[[paper]](https://arxiv.org/abs/2107.01980)
* 2021	The Story in Your Eyes: An Individual-difference-aware Model for Cross-person Gaze Estimation	[[paper]](https://arxiv.org/abs/2106.14183),	[[code]](https://github.com/bjj9/EVE_SCPT)	
* 2021	[Puregaze]	Puregaze: Purifying gaze feature for generalizable gaze estimation	[[paper]](https://arxiv.org/abs/2103.13173),	[[code]](https://github.com/yihuacheng/puregaze)	
* 2021	HybridGazeNet	HybridGazeNet: Geometric model guided Convolutional Neural Networks for gaze estimation	[[paper]](https://arxiv.org/abs/2111.11691)		
* 2022		Learning-by-Novel-View-Synthesis for Full-Face Appearance-based 3D Gaze Estimation	[[paper]](https://arxiv.org/abs/2201.07927)		
* 2022	[MTGLS]	MTGLS: Multi-Task Gaze Estimation with Limited Supervision	[[paper]](https://openaccess.thecvf.com/content/WACV2022/html/Ghosh_MTGLS_Multi-Task_Gaze_Estimation_With_Limited_Supervision_WACV_2022_paper.html)		
* 2018		Recurrent cnn for 3d gaze estimation using appearance and shape cues  [[paper]](Recurrent cnn for 3d gaze estimation using appearance and shape cues)		
* 2019		Neuro-inspired eye tracking with eye movement dynamics	[[paper]](https://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Neuro-Inspired_Eye_Tracking_With_Eye_Movement_Dynamics_CVPR_2019_paper.html)
* 2019		Realtime and accurate 3d eye gaze capture with dcnn-based iris and pupil segmentation	[[paper]](https://ieeexplore.ieee.org/abstract/document/8818661)
* 2019		Learning a 3d gaze estimator with improved itracker combined with bidirectional lstm  [[paper]](https://ieeexplore.ieee.org/abstract/document/8784770)				
* 2019	[Gaze360]	Gaze360: Physically unconstrained gaze estimation in the wild	[[paper]](http://gaze360.csail.mit.edu/iccv2019_gaze360.pdf),	[[code]](https://github.com/Erkil1452/gaze360),	[[code2]](https://github.com/erkil1452/gaze360/tree/master/code),	[[project]](http://gaze360.csail.mit.edu/)
* 2020  A Coarse-to-Fine Adaptive Network for Appearance-Based Gaze Estimation [[paper]](https://ojs.aaai.org/index.php/AAAI/article/view/6636),
* 2020	Towards end-to-end video-based eye-tracking	[[paper]](https://arxiv.org/abs/2007.13120),	[[code]](https://github.com/swook/EVE)		
* 2022	[Puregaze]	Puregaze: Purifying gaze feature for generalizable gaze estimation	[[paper]](https://ojs.aaai.org/index.php/AAAI/article/view/19921),	[[code]](https://github.com/yihuacheng/PureGaze)	
* 2022	[HAZE-Net]	HAZE-Net: High-Frequency Attentive Super-Resolved Gaze Estimation in Low-Resolution Face Images	[[paper]](https://arxiv.org/abs/2209.10167),	[[code]](https://github.com/dbseorms16/haze_net)		
* 2022	[LatentGaze]	LatentGaze: Cross-Domain Gaze Estimation through Gaze-Aware Analytic Latent Code Manipulation	[[paper]](https://arxiv.org/abs/2209.10171),	[[code]](https://github.com/leeisack/latentgaze)		


* 2020	AAAI	A Coarse-to-fine Adaptive Network for Appearance-based Gaze Estimation				[[paper]](https://ojs.aaai.org/index.php/AAAI/article/view/6636),			[[project]](https://paperswithcode.com/paper/a-coarse-to-fine-adaptive-network-for)
* 2022		iMon: Appearance-based Gaze Tracking System on Mobile Devices				[[paper]](https://ink.library.smu.edu.sg/cgi/viewcontent.cgi?article=7711&context=sis_research)
* 2021		GAZEL: Runtime Gaze Tracking for Smartphones [with Android Firebase]		[[paper]](https://ieeexplore.ieee.org/abstract/document/9439113),	[[github]](https://github.com/joonb14/GAZEL)
* 2020		Gaze Tracking and Point Estimation Using Low-Cost Head-Mounted Devices 	[[paper]](https://www.mdpi.com/1424-8220/20/7/1917)

* 2020		A Scenario-based Analysis of Front-facing Camera Eye Tracker for UX-UI Survey on Mobile Banking App		[[paper]](https://www.researchgate.net/publication/340551384_A_Scenario-based_Analysis_of_Front-facing_Camera_Eye_Tracker_for_UX-UI_Survey_on_Mobile_Banking_App)
* 2020		Gestatten: Estimation of User's Attention in Mobile MOOCs From Eye Gaze and Gaze Gesture Tracking		[[paper]](https://dl.acm.org/doi/10.1145/3394974),		[[Video]](https://www.youtube.com/watch?v=edMFZH90iB8)
* 2024		Smartphone-based Eye Tracking System using Edge Intelligence and Model Optimisation			[[paper]](https://arxiv.org/pdf/2408.12463)
* 2024	CVPR	Sharingan: A Transformer Architecture for Multi-Person Gaze Following		[[paper]](https://openaccess.thecvf.com/content/CVPR2024/papers/Tafasca_Sharingan_A_Transformer_Architecture_for_Multi-Person_Gaze_Following_CVPR_2024_paper.pdf),			[[project]](https://paperswithcode.com/paper/sharingan-a-transformer-architecture-for), [[supp]](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Tafasca_Sharingan_A_Transformer_CVPR_2024_supplemental.pdf)
* 2024	CVPR	From Feature to Gaze: A Generalizable Replacement of Linear Layer for Gaze Estimation				[[paper]](https://openaccess.thecvf.com/content/CVPR2024/papers/Bao_From_Feature_to_Gaze_A_Generalizable_Replacement_of_Linear_Layer_CVPR_2024_paper.pdf)
* 2024	CVPR	What Do You See in Vehicle? Comprehensive Vision Solution for In-Vehicle Gaze Estimation		[[paper]](https://openaccess.thecvf.com/content/CVPR2024/papers/Bao_From_Feature_to_Gaze_A_Generalizable_Replacement_of_Linear_Layer_CVPR_2024_paper.pdf),	[[code]](https://github.com/yihuacheng/IVGaze),		[[project]](https://yihua.zone/work/ivgaze/), [[supp]](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Cheng_What_Do_You_CVPR_2024_supplemental.pdf)
* 2024	CVPR	Unsupervised Gaze Representation Learning from Multi-view Face Images				[[paper]](https://openaccess.thecvf.com/content/CVPR2024/papers/Bao_Unsupervised_Gaze_Representation_Learning_from_Multi-view_Face_Images_CVPR_2024_paper.pdf),	[[project]](https://paperswithcode.com/paper/unsupervised-gaze-representation-learning)
* 2024	CVPR	Learning from Observer Gaze: Zero-Shot Attention Prediction Oriented by Human-Object Interaction Recognition			[[paper]](https://openaccess.thecvf.com/content/CVPR2024/html/Zhou_Learning_from_Observer_Gaze_Zero-Shot_Attention_Prediction_Oriented_by_Human-Object_CVPR_2024_paper.html),		[[project]](https://yuchen2199.github.io/Interactive-Gaze/)
* 2023	CVPR	Gazeformer: Scalable, Effective and Fast Prediction of Goal-Directed Human Attention	[[paper]](https://openaccess.thecvf.com/content/CVPR2023/papers/Mondal_Gazeformer_Scalable_Effective_and_Fast_Prediction_of_Goal-Directed_Human_Attention_CVPR_2023_paper.pdf),	[[Code]](https://github.com/cvlab-stonybrook/Gazeformer/),	[[arXiv]](https://arxiv.org/abs/2303.15274),  [[supp]](https://openaccess.thecvf.com/content/CVPR2023/supplemental/Mondal_Gazeformer_Scalable_Effective_CVPR_2023_supplemental.pdf)
* 2023	CVPR	GazeNeRF: 3D-Aware Gaze Redirection with Neural Radiance Fields	pre-trained			[[paper]](https://arxiv.org/pdf/2212.04823),	[[Code]](https://github.com/AlessandroRuzzi/GazeNeRF),	[[Video]](https://www.youtube.com/watch?v=JwqKbmUR3DE),	[[project]](https://x-shi.github.io/GazeNeRF.github.io/)
* 2023	CVPR	Source-Free Adaptive Gaze Estimation by Uncertainty Reduction		[[paper]](https://openaccess.thecvf.com/content/CVPR2023/papers/Cai_Source-Free_Adaptive_Gaze_Estimation_by_Uncertainty_Reduction_CVPR_2023_paper.pdf), 	[[code]](https://github.com/caixin1998/UnReGA)
* 2023	CVPR	ReDirTrans: Latent-to-Latent Translation for Gaze and Head Redirection	[[paper]](https://openaccess.thecvf.com/content/CVPR2023/papers/Jin_ReDirTrans_Latent-to-Latent_Translation_for_Gaze_and_Head_Redirection_CVPR_2023_paper.pdf),	[[code]](https://github.com/caixin1998/UnReGA), [[supp]](https://openaccess.thecvf.com/content/CVPR2023/supplemental/Jin_ReDirTrans_Latent-to-Latent_Translation_CVPR_2023_supplemental.pdf)
* 2022	CVPR	Contrastive Regression for Domain Adaptation on Gaze Estimation			[[paper]](https://openaccess.thecvf.com/content/CVPR2022/html/Wang_Contrastive_Regression_for_Domain_Adaptation_on_Gaze_Estimation_CVPR_2022_paper.html), [[supp]](https://openaccess.thecvf.com/content/CVPR2022/supplemental/Wang_Contrastive_Regression_for_CVPR_2022_supplemental.pdf)
* 2022	CVPR	GazeOnce: Real-Time Multi-Person Gaze Estimation		[[paper]](https://openaccess.thecvf.com/content/CVPR2022/html/Zhang_GazeOnce_Real-Time_Multi-Person_Gaze_Estimation_CVPR_2022_paper.html),	[[code]](https://github.com/mf-zhang/GazeOnce)
* 2022	CVPR	ESCNet: Gaze Target Detection With the Understanding of 3D Scenes		[[paper]](https://openaccess.thecvf.com/content/CVPR2022/html/Bao_ESCNet_Gaze_Target_Detection_With_the_Understanding_of_3D_Scenes_CVPR_2022_paper.html),	[[code]](https://github.com/bjj9/ESCNet), [[supp]](https://openaccess.thecvf.com/content/CVPR2022/supplemental/Bao_ESCNet_Gaze_Target_CVPR_2022_supplemental.pdf)
* 2022	CVPR	Generalizing Gaze Estimation With Rotation Consistency				[[paper]](https://openaccess.thecvf.com/content/CVPR2022/html/Bao_Generalizing_Gaze_Estimation_With_Rotation_Consistency_CVPR_2022_paper.html)
* 2022	CVPR	GaTector: A Unified Framework for Gaze Object Prediction		[[paper]](https://openaccess.thecvf.com/content/CVPR2022/html/Wang_GaTector_A_Unified_Framework_for_Gaze_Object_Prediction_CVPR_2022_paper.html) 			
* 2022	CVPR	Dynamic 3D Gaze From Afar: Deep Gaze Estimation From Temporal Eye-Head-Body Coordination			[[paper]](https://openaccess.thecvf.com/content/CVPR2022/html/Nonaka_Dynamic_3D_Gaze_From_Afar_Deep_Gaze_Estimation_From_Temporal_CVPR_2022_paper.html),		[[arXiv]](https://arxiv.org/abs/2203.10433)
* 2022	CVPR	End-to-End Human-Gaze-Target Detection With Transformers	[[paper]](https://openaccess.thecvf.com/content/CVPR2022/html/Tu_End-to-End_Human-Gaze-Target_Detection_With_Transformers_CVPR_2022_paper.html),	[[code]](https://github.com/francescotonini/human-gaze-target-detection-transformer),	[[arXiv]](https://arxiv.org/abs/2203.10433)
* 2021	CVPR	Weakly-Supervised Physically Unconstrained Gaze Estimation		[[paper]](https://openaccess.thecvf.com/content/CVPR2021/html/Kothari_Weakly-Supervised_Physically_Unconstrained_Gaze_Estimation_CVPR_2021_paper.html),	[[code]](https://github.com/NVlabs/weakly-supervised-gaze),	[[arXiv]](https://arxiv.org/abs/2105.09803), [[supp]](https://openaccess.thecvf.com/content/CVPR2021/supplemental/Kothari_Weakly-Supervised_Physically_Unconstrained_CVPR_2021_supplemental.pdf)
* 2021	CVPR	Goal-Oriented Gaze Estimation for Zero-Shot Learning	[[paper]](https://openaccess.thecvf.com/content/CVPR2021/html/Liu_Goal-Oriented_Gaze_Estimation_for_Zero-Shot_Learning_CVPR_2021_paper.html),	[[code]](https://github.com/francescotonini/human-gaze-target-detection-transformer),	[[arXiv]](https://arxiv.org/abs/2203.10433)
* 2021	CVPR	Glance and Gaze: Inferring Action-Aware Points for One-Stage Human-Object Interaction Detection	[[paper]](https://openaccess.thecvf.com/content/CVPR2021/html/Zhong_Glance_and_Gaze_Inferring_Action-Aware_Points_for_One-Stage_Human-Object_Interaction_CVPR_2021_paper.html),	[[code]](https://github.com/SherlockHolmes221/GGNet),	[[arXiv]](https://arxiv.org/abs/2104.05269), [[supp]](https://openaccess.thecvf.com/content/CVPR2021/supplemental/Zhong_Glance_and_Gaze_CVPR_2021_supplemental.pdf)
* 2021	CVPR	Dual Attention Guided Gaze Target Detection in the Wild	 [[paper]](https://openaccess.thecvf.com/content/CVPR2021/html/Fang_Dual_Attention_Guided_Gaze_Target_Detection_in_the_Wild_CVPR_2021_paper.html)
* 2022	OWLET: An automated, open-source method for infant gaze tracking using smartphone and webcam recordings	[[paper]](https://www.researchgate.net/publication/363358846_OWLET_An_automated_open-source_method_for_infant_gaze_tracking_using_smartphone_and_webcam_recordings),  [[code]](https://github.com/denisemw/OWLET),  [[info]](https://britobabylab.com/wp-content/uploads/2023/01/OWLET-1.pdf),  [[project]](https://www.brainnexus.com/projects-2/owlet%3A-a-tool-for-infant-eye-tracking-via-zoom-recordings)
* 2023	iCatcher+: Robust and Automated Annotation of Infants’ and Young Children’s Gaze Behavior From Videos Collected in Laboratory, Field, and Online Studies  [[paper]](https://www.researchgate.net/publication/370134556_iCatcher_Robust_and_Automated_Annotation_of_Infants'_and_Young_Children's_Gaze_Behavior_From_Videos_Collected_in_Laboratory_Field_and_Online_Studies)
	
* 2022		Analysis and novel methods for capture of normative eye-tracking data in 2.5-month old infants	[[paper]](https://www.researchgate.net/publication/366161948_Analysis_and_novel_methods_for_capture_of_normative_eye-tracking_data_in_25-month_old_infants)  
* 2023		Deep-SAGA: a deep-learning-based system for automatic gaze annotation from eye-tracking data	[[paper]](https://www.researchgate.net/publication/361019174_Deep-SAGA_a_deep-learning-based_system_for_automatic_gaze_annotation_from_eye-tracking_data#fullTextFileContent),	[[code]](https://github.com/OliDeane/Deep_SAGA) 
* 2023		Real-time camera-based eye gaze tracking using convolutional neural network: a case study on social media website [[paper]](https://www.researchgate.net/publication/359608492_Real-time_camera-based_eye_gaze_tracking_using_convolutional_neural_network_a_case_study_on_social_media_website)
 
* 2021		A Deep Learning-Based Approach to Video-Based Eye Tracking for Human Psychophysics   [[paper]](https://www.frontiersin.org/journals/human-neuroscience/articles/10.3389/fnhum.2021.685830/full)
* 2023		Validation of an Open Source, Remote Web-based Eye-tracking Method (WebGazer) for Research in Early Childhood	[[paper]](https://www.researchgate.net/publication/374821818_Validation_of_an_open_source_remote_web-based_eye-tracking_method_WebGazer_for_research_in_early_childhood),  [[paper]](https://osf.io/preprints/psyarxiv/7924h)
* 2023		Webcam eye tracking close to laboratory standards: Comparing a new webcam-based system and the EyeLink 1000	[[paper]](https://www.researchgate.net/publication/374659238_Webcam_eye_tracking_close_to_laboratory_standards_Comparing_a_new_webcam-based_system_and_the_EyeLink_1000)
* 2021	ICCV	Cross-Encoder for Unsupervised Gaze Representation Learning   [[paper]](https://openaccess.thecvf.com/content/ICCV2021/papers/Sun_Cross-Encoder_for_Unsupervised_Gaze_Representation_Learning_ICCV_2021_paper.pdf),	[[code]](https://github.com/sunyunjia96/Cross-Encoder), [[poster]](https://dualplus.github.io/publication/2021-iccv-crossencoder/poster.pdf)
* 2022	CVPRW	Unsupervised Multi-View Gaze Representation Learning	[[paper]](https://openaccess.thecvf.com/content/CVPR2022W/GAZE/papers/Gideon_Unsupervised_Multi-View_Gaze_Representation_Learning_CVPRW_2022_paper.pdf),  [[code]](https://github.com/ToyotaResearchInstitute/UnsupervisedGaze)
* Vulnerability of Appearance-based Gaze Estimation	[[paper]](https://arxiv.org/pdf/2103.13134)
* 2021	ICCV	Generalizing Gaze Estimation with Outlier-guided Collaborative Adaptation  [[paper]](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Generalizing_Gaze_Estimation_With_Outlier-Guided_Collaborative_Adaptation_ICCV_2021_paper.pdf),	[[code]](https://github.com/DreamtaleCore/PnP-GA)


* 2021	CVPRW	Goo: A Dataset for Gaze Object Prediction in Retail Environments  [[paper]](https://arxiv.org/pdf/2105.10793),	[[code]](https://github.com/upeee/GOO-GAZE2021)  
* 2022	CVPRW	A Modular Multimodal Architecture for Gaze Target Prediction: Application to Privacy-Sensitive Settings	[[paper]](https://openaccess.thecvf.com/content/CVPR2022W/GAZE/papers/Gupta_A_Modular_Multimodal_Architecture_for_Gaze_Target_Prediction_Application_to_CVPRW_2022_paper.pdf),	[[code]](https://github.com/idiap/multimodal_gaze_target_prediction)
* 2020	CVPR	Detecting Attended Visual Targets in Video  [[paper]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Chong_Detecting_Attended_Visual_Targets_in_Video_CVPR_2020_paper.pdf),  [[code]](https://github.com/ejcgt/attention-target-detection),	[[arXiv]](https://arxiv.org/abs/2003.02501)
* 2020	WACV	Offset calibration for Appearance-based gaze estimation via gaze decomposition	[[paper]](https://openaccess.thecvf.com/content_WACV_2020/papers/Chen_Offset_Calibration_for_Appearance-Based_Gaze_Estimation_via_Gaze_Decomposition_WACV_2020_paper.pdf), [[code]](https://github.com/czk32611/Gaze-Decomposition),  [[arXiv]](https://arxiv.org/abs/1905.04451)		
* 2021		Bayesian Eye Tracking	[[paper]](https://arxiv.org/pdf/2106.13387)
* 2021	NeurIPS	Glance-and-Gaze Vision Transformer  [[paper]](https://arxiv.org/pdf/2106.02277),  [[code]](https://github.com/yucornetto/GG-Transformer),  [[project]](https://paperswithcode.com/paper/glance-and-gaze-vision-transformer)
* 2020	NeurIPS 	Self-supervised learning through the eyes of a child.	[[paper]](https://papers.nips.cc/paper/2020/file/7183145a2a3e0ce2b68cd3735186b1d5-Paper.pdf),	[[code]](https://github.com/eminorhan/baby-vision),	[[project]](https://paperswithcode.com/paper/self-supervised-learning-through-the-eyes-of)
* 2020	ECCV	ETH-XGaze: A Large Scale Dataset for Gaze Estimation under Extreme Head Pose and Gaze Variation. [[paper]](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123500358.pdf)  
* 2020	ECCV	Deep Learning-based Pupil Center Detection for Fast and Accurate Eye Tracking System. 	[[paper]](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123640035.pdf), [[project]](https://paperswithcode.com/paper/deep-learning-based-pupil-center-detection)
* 2022	NeurIPS 	Contrastive Representation Learning for Gaze Estimation	[[paper]](https://arxiv.org/abs/2210.13404),	[[code]](https://github.com/jswati31/gazeclr)
  	
* 2020	ACCV	Domain Adaptation Gaze Estimation by Embedding with Prediction Consistency  [[paper]](https://openaccess.thecvf.com/content/ACCV2020/papers/Guo_Domain_Adaptation_Gaze_Estimation_by_Embedding_with_Prediction_Consistency_ACCV_2020_paper.pdf)
* 2020	WACV	Gaze Estimation for Assisted Living Environments  [[paper]](https://openaccess.thecvf.com/content_WACV_2020/papers/Dias_Gaze_Estimation_for_Assisted_Living_Environments_WACV_2020_paper.pdf),	[[code]](https://bitbucket.org/phil_dias/gaze-estimation/src/master/),	[[project]](https://github.com/tkuri/papers/issues/301)
* 2020	WACV	Learning to Detect Head Movement in Unconstrained Remote Gaze Estimation in the Wild	[[paper]](https://openaccess.thecvf.com/content_WACV_2020/papers/Wang_Learning_to_Detect_Head_Movement_in_Unconstrained_Remote_Gaze_Estimation_WACV_2020_paper.pdf)	
* 2020	BMVC	Learning-based Region Selection for End-to-End Gaze Estimation	[[paper]](https://www.bmvc2020-conference.com/assets/papers/0086.pdf)			
* 2021	MDPI	Low Cost Gaze Estimation: Knowledge-Based Solutions.	[[paper]](https://www.mdpi.com/1424-8220/21/15/5109)			
* 2019	ICCV	FAZE:Few-Shot Adaptive Gaze Estimation	[[paper]](https://openaccess.thecvf.com/content_ICCV_2019/papers/Park_Few-Shot_Adaptive_Gaze_Estimation_ICCV_2019_paper.pdf),	[[code]](https://github.com/NVlabs/few_shot_gaze)		
* 2020	IEEE TIP	Gaze Estimation by Exploring Two-Eye Asymmetry							
* 2023	ICCV	DVGaze: Dual-View Gaze Estimation  [[paper]](https://openaccess.thecvf.com/content/ICCV2023/papers/Cheng_DVGaze_Dual-View_Gaze_Estimation_ICCV_2023_paper.pdf),	[[code]](https://github.com/yihuacheng/DVGaze)		
* 2019	CVPR	Mixed Effects Neural Networks (MeNets) with Applications to Gaze Estimation	[[paper]](https://openaccess.thecvf.com/content_CVPR_2019/papers/Xiong_Mixed_Effects_Neural_Networks_MeNets_With_Applications_to_Gaze_Estimation_CVPR_2019_paper.pdf),	[[code]](https://github.com/vsingh-group/MeNets)
* 2017	ACM	InvisibleEye: Mobile Eye Tracking Using Multiple Low-Resolution Cameras and Learning-Based Gaze Estimation  [[paper]](https://dl.acm.org/doi/10.1145/3130971)		
* 2016		A low-cost and calibration-free gaze estimator for soft biometrics: An explorative study [[paper]](https://www.semanticscholar.org/paper/A-low-cost-and-calibration-free-gaze-estimator-for-Cazzato-Evangelista/557a87d7d0f3a359dd4ced40dae9785db501fa56)
* 2024		Model-Based 3D Gaze Estimation Using a TOF Camera [[paper]](https://www.mdpi.com/1424-8220/24/4/1070)			
* 2019	AAAI	RGBD Based Gaze Estimation via Multi-task CNN	[[paper]](https://svip-lab.github.io/paper/aaai2019_liandz1.pdf),  [[code]](https://github.com/svip-lab/RGBD-Gaze)		
* 2018	TNNLS	Multi-view Multi-task Gaze Estimation with Deep Convolutional Neural Networks	[[paper]](https://ieeexplore.ieee.org/document/8454246),	[[code]](https://github.com/dongzelian/multi-view-gaze)		
* 2016		Fast and Accurate Algorithm for Eye Localization for Gaze Tracking in Low Resolution Images	[[paper]](https://arxiv.org/abs/1605.05272),	[[project]](https://paperswithcode.com/paper/fast-and-accurate-algorithm-for-eye)
* 2017	IEEE TIP	Appearance-Based Gaze Estimation via Uncalibrated Gaze Pattern Recovery	[[paper]](https://www.researchgate.net/publication/312926555_Appearance-Based_Gaze_Estimation_via_Uncalibrated_Gaze_Pattern_Recovery)			
* 2022	ACCV	‘Labelling the Gaps’: A Weakly Supervised Automatic Eye Gaze Estimation	 [[paper]](https://arxiv.org/abs/2208.01840), [[code]](https://github.com/i-am-shreya/Labelling-the-Gaps), [[paper]](https://openaccess.thecvf.com/content/ACCV2022/papers/Ghosh_Labelling_the_Gaps_A_Weakly_Supervised_Automatic_Eye_Gaze_Estimation_ACCV_2022_paper.pdf)	
* 2021	CVPRW	Appearance-based Gaze Estimation using Attention and Difference Mechanism  [[paper]](https://openaccess.thecvf.com/content/CVPR2021W/GAZE/papers/D_Appearance-Based_Gaze_Estimation_Using_Attention_and_Difference_Mechanism_CVPRW_2021_paper.pdf)			


* 2024	GAZE	Spatio-Temporal Attention and Gaussian Processes for Personalized Video Gaze Estimation	 [[paper]](https:/openaccess.thecvf.com/content/CVPR2024W/GAZE/papers/Jindal_Spatio-Temporal_Attention_and_Gaussian_Processes_for_Personalized_Video_Gaze_Estimation_CVPRW_2024_paper.pdf)	[[code]](https://github.com/jswati31/stage)	[[arXiv]](https://arxiv.org/abs/2404.05215), [[supp]](https:/openaccess.thecvf.com/content/CVPR2024W/GAZE/supplemental/Jindal_Spatio-Temporal_Attention_and_CVPRW_2024_supplemental.pdf)	
* 2024	GAZE	Exploring the Zero-Shot Capabilities of Vision-Language Models for Improving Gaze Following	[[paper]](https:/openaccess.thecvf.com/content/CVPR2024W/GAZE/papers/Gupta_Exploring_the_Zero-Shot_Capabilities_of_Vision-Language_Models_for_Improving_Gaze_CVPRW_2024_paper.pdf)				[[supp]](https:/openaccess.thecvf.com/content/CVPR2024W/GAZE/supplemental/Gupta_Exploring_the_Zero-Shot_CVPRW_2024_supplemental.pdf)
* 2024	GAZE    Gaze Scanpath Transformer: Predicting Visual Search Target by Spatiotemporal Semantic Modeling of Gaze Scanpath	[[paper]](https:/openaccess.thecvf.com/content/CVPR2024W/GAZE/papers/Nishiyasu_Gaze_Scanpath_Transformer_Predicting_Visual_Search_Target_by_Spatiotemporal_Semantic_CVPRW_2024_paper.pdf)
* 2024	GAZE    GESCAM: A Dataset and Method on Gaze Estimation for Classroom Attention Measurement [[paper]](https:/openaccess.thecvf.com/content/CVPR2024W/GAZE/papers/Mathew_GESCAM__A_Dataset_and_Method_on_Gaze_Estimation_for_CVPRW_2024_paper.pdf)			[[project]](https://athulmmathew.github.io/GESCAM/)
  
* 2023	GAZE	Where are they looking in the 3D space?				[[paper]](https:/openaccess.thecvf.com/content/CVPR2023W/GAZE/papers/Horanyi_Where_Are_They_Looking_in_the_3D_Space_CVPRW_2023_paper.pdf)				[[supp]](https:/openaccess.thecvf.com/content/CVPR2023W/GAZE/supplemental/Horanyi_Where_Are_They_CVPRW_2023_supplemental.pdf)	
* 2023	GAZE	Kappa Angle Regression with Ocular Counter-Rolling Awareness for Gaze Estimation		[[paper]](https://openaccess.thecvf.com/content/CVPR2023W/GAZE/papers/Jin_Kappa_Angle_Regression_With_Ocular_Counter-Rolling_Awareness_for_Gaze_Estimation_CVPRW_2023_paper.pdf)				[[supp]](https:/openaccess.thecvf.com/content/CVPR2023W/GAZE/supplemental/Jin_Kappa_Angle_Regression_CVPRW_2023_supplemental.pdf)	
* 2023	GAZE	EFE: End-to-end Frame-to-Gaze Estimation		[[paper]](https:/openaccess.thecvf.com/content/CVPR2023W/GAZE/papers/Balim_EFE_End-to-End_Frame-To-Gaze_Estimation_CVPRW_2023_paper.pdf)				[[arXiv]](https://arxiv.org/abs/2305.05526)	
* 2023	GAZE	GazeCaps: Gaze Estimation with Self-Attention-Routed Capsules				[[paper]](https:/openaccess.thecvf.com/content/CVPR2023W/GAZE/papers/Wang_GazeCaps_Gaze_Estimation_With_Self-Attention-Routed_Capsules_CVPRW_2023_paper.pdf)				[[supp]](https:/openaccess.thecvf.com/content/CVPR2023W/GAZE/supplemental/Wang_GazeCaps_Gaze_Estimation_CVPRW_2023_supplemental.pdf)	

* 2022	GAZE	Learning-by-Novel-View-Synthesis for Full-Face Appearance-Based 3D Gaze Estimation				[[paper]](https:/openaccess.thecvf.com/content/CVPR2022W/GAZE/papers/Qin_Learning-by-Novel-View-Synthesis_for_Full-Face_Appearance-Based_3D_Gaze_Estimation_CVPRW_2022_paper.pdf)		[[video]](https://www.youtube.com/watch?v=BUFTzo5DqXc)		[[supp]](https:/openaccess.thecvf.com/content/CVPR2022W/GAZE/supplemental/Qin_Learning-by-Novel-View-Synthesis_for_Full-Face_CVPRW_2022_supplemental.pdf)	[[arXiv]](https://arxiv.org/abs/2201.07927)
* 2022	GAZE	Self-Attention with Convolution and Deconvolution for Efficient Eye Gaze Estimation from a Full Face Image	[[paper]](https:/openaccess.thecvf.com/content/CVPR2022W/GAZE/papers/Oh_Self-Attention_With_Convolution_and_Deconvolution_for_Efficient_Eye_Gaze_Estimation_CVPRW_2022_paper.pdf)		[[video]](https://www.youtube.com/watch?v=ANQ65NNNWNE)			
* 2022	GAZE	Unsupervised Multi-View Gaze Representation Learning			[[paper]](https:/openaccess.thecvf.com/content/CVPR2022W/GAZE/papers/Gideon_Unsupervised_Multi-View_Gaze_Representation_Learning_CVPRW_2022_paper.pdf)		[[video]](https://www.youtube.com/watch?v=W0OK1vVtiEk)			
* 2022	GAZE	ScanpathNet: A Recurrent Mixture Density Network for Scanpath Prediction	[[paper]](https:/openaccess.thecvf.com/content/CVPR2022W/GAZE/papers/de_Belen_ScanpathNet_A_Recurrent_Mixture_Density_Network_for_Scanpath_Prediction_CVPRW_2022_paper.pdf)		[[video]](https://www.youtube.com/watch?v=8RXog3XkCl8)		[[supp]](https:/openaccess.thecvf.com/content/CVPR2022W/GAZE/supplemental/de_Belen_ScanpathNet_A_Recurrent_CVPRW_2022_supplemental.pdf)	
* 2022	GAZE	One-Stage Object Referring with Gaze Estimation				[[paper]](https:/openaccess.thecvf.com/content/CVPR2022W/GAZE/papers/Chen_One-Stage_Object_Referring_With_Gaze_Estimation_CVPRW_2022_paper.pdf)		[[video]](https://www.youtube.com/watch?v=SkjtCXX-aJY)			
* 2022	GAZE	Characterizing Target-absent Human Attention				[[paper]](https:/openaccess.thecvf.com/content/CVPR2022W/GAZE/papers/Chen_Characterizing_Target-Absent_Human_Attention_CVPRW_2022_paper.pdf)		[[video]](https://www.youtube.com/watch?v=SIVywYz2pNs)		[[supp]](https:/openaccess.thecvf.com/content/CVPR2022W/GAZE/supplemental/Chen_Characterizing_Target-Absent_Human_CVPRW_2022_supplemental.pdf)	
* 2022	GAZE	A Modular Multimodal Architecture for Gaze Target Prediction: Application to Privacy-Sensitive Settings				[[paper]](https:/openaccess.thecvf.com/content/CVPR2022W/GAZE/papers/Gupta_A_Modular_Multimodal_Architecture_for_Gaze_Target_Prediction_Application_to_CVPRW_2022_paper.pdf)		[[video]](https://www.youtube.com/watch?v=z-XSwLOpNzw)			
											


# Other codes
* https://github.com/antoinelame/GazeTracking
* https://github.com/iitmcvg/eye-gaze
* 2021	pl_gaze_estimation
        https://github.com/hysts/pl_gaze_estimation
  
	https://github.com/hysts/pytorch_mpiigaze

	https://github.com/hysts/pytorch_mpiigaze_demo
* 2023	detect what product a shopper looks at in a retail store using single-view RGB images from a CCTV 	https://github.com/Varun-Tandon14/Gazed-at-object-prediction-in-retail-environment
* 2022	Pupil: Open source eye tracking platform	https://github.com/pupil-labs/pupil
* 2023	Awesome Work on Gaze Estimation	https://github.com/cvlab-uob/Awesome-Gaze-Estimation
* 2020	Gaze Estimation with Deep Learning	https://github.com/david-wb/gaze-estimation
* 2019	Awesome Work on Gaze Estimation	https://github.com/jinsz/awesome-gaze-estimation
* 2024	various gaze estimation techniques	https://github.com/i-am-shreya/Eye-Gaze-Survey?tab=readme-ov-file#Eye-Gaze-Estimation
* 2018	Gaze Estimation using Deep Learning, a Tensorflow-based framework.	https://github.com/swook/GazeML
* 2022	GazeEstimation-Summary	https://github.com/yihuacheng/GazeEstimation-Summary
* 2019	eye localization and gaze tracking system for a single low cost camera	https://github.com/ricber/Gaze-Tracker
* 2019	A webcam based pupil tracking implementation	https://github.com/trishume/eyeLike
* 2022	Gaze Tracking	https://github.com/antoinelame/GazeTracking
  
	Eye Tracking library 	https://github.com/antoinelame/GazeTracking

	Repository for Eye Gaze Detection and Tracking	https://github.com/iitmcvg/eye-gaze
		
